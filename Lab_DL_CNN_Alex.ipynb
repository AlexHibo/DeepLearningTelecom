{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaht-FPA1Jvq"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "## Lab2: Train a Convolutional Neural Network (CNN).\n",
        "\n",
        "In this Lab session we will learn how to train a CNN from scratch for classifying MNIST digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "UvxtTYHlVfRK"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYCvhGxKWyN7"
      },
      "source": [
        "### Define LeNet\n",
        "\n",
        "![network architecture](https://www.researchgate.net/profile/Lucijano-Berus/publication/329891470/figure/fig1/AS:707347647307776@1545656229128/Architecture-of-LeNet-5-a-Convolutional-Neural-Network-for-digits-digits-recognition-An.ppm)\n",
        "\n",
        "Here we are going to define our first CNN which is **LeNet** in this case. This architecture has been introduced and is detailed in [this article](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf). To construct a LeNet we will be using some convolutional layers followed by some fully-connected layers. The convolutional layers can be simply defined using `torch.nn.Conv2d` module of `torch.nn` package. Details can be found [here](https://pytorch.org/docs/stable/nn.html#conv2d). Moreover, we will use pooling operation to reduce the size of convolutional feature maps. For this case we are going to use `torch.nn.functional.max_pool2d`. Details about maxpooling can be found [here](https://pytorch.org/docs/stable/nn.html#max-pool2d)\n",
        "\n",
        "Differently from our previous Lab, we will use a Rectified Linear Units (ReLU) as activation function with the help of `torch.nn.functional.relu`, replacing `torch.nn.Sigmoid`. Details about ReLU can be found [here](https://pytorch.org/docs/stable/nn.html#id26)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMC_LDYdWkI7"
      },
      "outputs": [],
      "source": [
        "class LeNet(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNet, self).__init__()\n",
        "\n",
        "    # input channel =1 , output channels = 6, kernel size = 5\n",
        "    # input image size = (28, 28) and not 32*32, image output size = (24, 24)\n",
        "    # TODO\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
        "\n",
        "    # input channel = 6, output channels = 16, kernel size = 5\n",
        "    # input image size = (12, 12), output image size = (8, 8)\n",
        "    # TODO\n",
        "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
        "\n",
        "\n",
        "    # input dim = 4*4*16 ( H x W x C), output dim = 120\n",
        "    # TODO\n",
        "    self.fc1 = nn.Linear(in_features=4*4*16, out_features=120)\n",
        "\n",
        "    # input dim = 120, output dim = 84\n",
        "    # TODO\n",
        "    self.fc2 = nn.Linear(in_features=120,out_features=84)\n",
        "\n",
        "    # input dim = 84, output dim = 10\n",
        "    # TODO\n",
        "    self.fc3 = nn.Linear(in_features=84,out_features=10)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # TODO\n",
        "    x = F.relu(self.conv1(x))\n",
        "    # Max Pooling with kernel size = 2 (and stride = 2)\n",
        "    # output size = (12, 12)\n",
        "    # TODO\n",
        "    x = F.max_pool2d(x, 2)\n",
        "\n",
        "    # TODO\n",
        "    x = F.relu(self.conv2(x))\n",
        "    # Max Pooling with kernel size = 2 (and stride = 2)\n",
        "    # output size = (4, 4)\n",
        "    # TODO\n",
        "    x = F.max_pool2d(x, 2)\n",
        "\n",
        "    # flatten the feature maps into a long vector\n",
        "    x = x.view(x.shape[0], -1)\n",
        "\n",
        "    # TODO\n",
        "    x = F.relu(self.fc1(x))          # output size: (batch, 120)\n",
        "    # TODO\n",
        "    x = F.relu(self.fc2(x))          # output size: (batch, 84)\n",
        "    # TODO\n",
        "    x = self.fc3(x)                  # output size: (batch, 10), without activation\n",
        "    \n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gChf6TvWonrV"
      },
      "source": [
        "### Define cost function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "6j5UrBH3oek8"
      },
      "outputs": [],
      "source": [
        "def get_cost_function():\n",
        "  cost_function = nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2TjXeVdorV9"
      },
      "source": [
        "### Define the optimizer\n",
        "\n",
        "We will use SGD with learning rate-lr, weight_decay=wd and  momentum=momentum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "hBZN-WPboulR"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTkfrV64oxIL"
      },
      "source": [
        "### Train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-sE5vFio0lf"
      },
      "outputs": [],
      "source": [
        "def test(net, data_loader, cost_function, device='cpu'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # Load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # Apply the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # Better print something\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "\n",
        "def train(net,data_loader,optimizer,cost_function, device='cpu'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "\n",
        "  net.train() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # Load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # Apply the loss\n",
        "    loss = cost_function(outputs,targets)\n",
        "\n",
        "    # Reset the optimizer\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Better print something, no?\n",
        "    samples+=inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6IT0Lsgo8AM"
      },
      "source": [
        "### Define the function that fetches a data loader that is then used during iterative training.\n",
        "\n",
        "We will learn a new thing in this function as how to Normalize the inputs given to the network.\n",
        "\n",
        "***Why Normalization is needed***?\n",
        "\n",
        "To have nice and stable training of the network it is recommended to normalize the network inputs between \\[-1, 1\\].\n",
        "\n",
        "***How it can be done***?\n",
        "\n",
        "This can be simply done using `torchvision.transforms.Normalize()` transform. Details can be found [here](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Normalize)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "qDxpo6uVo_8k"
      },
      "outputs": [],
      "source": [
        "def get_data(batch_size, test_batch_size=256):\n",
        "\n",
        "  # Prepare data transformations and then combine them sequentially\n",
        "  transform = list()\n",
        "  transform.append(T.ToTensor())                            # converts Numpy to Pytorch Tensor\n",
        "  transform.append(T.Normalize(mean=[0.5], std=[0.5]))      # Normalizes the Tensors between [-1, 1]\n",
        "  transform = T.Compose(transform)                          # Composes the above transformations into one.\n",
        "\n",
        "  # Load data\n",
        "  full_training_data = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True)\n",
        "  test_data = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True)\n",
        "\n",
        "\n",
        "  # Create train and validation splits\n",
        "  num_samples = len(full_training_data)\n",
        "  training_samples = int(num_samples*0.5+1)\n",
        "  validation_samples = num_samples - training_samples\n",
        "\n",
        "  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
        "\n",
        "  # Initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n",
        "\n",
        "  return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHcB8f0AsY4n"
      },
      "source": [
        "### Wrapping everything up\n",
        "\n",
        "Finally, we need a main function which initializes everything + the needed hyperparameters and loops over multiple epochs (printing the results)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip_R-hruse0Q"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Input arguments\n",
        "  batch_size: Size of a mini-batch\n",
        "  device: GPU where you want to train your network\n",
        "  weight_decay: Weight decay co-efficient for regularization of weights\n",
        "  momentum: Momentum for SGD optimizer\n",
        "  epochs: Number of epochs for training the network\n",
        "'''\n",
        "\n",
        "def main(batch_size=128,\n",
        "         device='cpu',\n",
        "         learning_rate=0.01,\n",
        "         weight_decay=0.000001,\n",
        "         momentum=0.9,\n",
        "         epochs=50,\n",
        "         lenet='1'):\n",
        "\n",
        "  train_loader, val_loader, test_loader = get_data(batch_size)\n",
        "\n",
        "  # TODO for defining LeNet-5 and moving it to the GPU\n",
        "  if lenet=='1':\n",
        "    net = LeNet().to(device)\n",
        "  elif lenet=='5':\n",
        "    net = LeNet5().to(device)\n",
        "  else :\n",
        "    print('No net Found')\n",
        "    return\n",
        "\n",
        "\n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "  return net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltdCMiB3t18h"
      },
      "source": [
        "Lets train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "6d-z20H4tziL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before training:\n",
            "\t Training loss 0.01806, Training accuracy 11.26\n",
            "\t Validation loss 0.00907, Validation accuracy 11.22\n",
            "\t Test loss 0.00923, Test accuracy 11.35\n",
            "-----------------------------------------------------\n",
            "Epoch: 1\n",
            "\t Training loss 0.00938, Training accuracy 58.31\n",
            "\t Validation loss 0.00103, Validation accuracy 92.05\n",
            "-----------------------------------------------------\n",
            "Epoch: 2\n",
            "\t Training loss 0.00133, Training accuracy 94.70\n",
            "\t Validation loss 0.00062, Validation accuracy 94.98\n",
            "-----------------------------------------------------\n",
            "Epoch: 3\n",
            "\t Training loss 0.00083, Training accuracy 96.66\n",
            "\t Validation loss 0.00036, Validation accuracy 97.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 4\n",
            "\t Training loss 0.00061, Training accuracy 97.61\n",
            "\t Validation loss 0.00036, Validation accuracy 97.23\n",
            "-----------------------------------------------------\n",
            "Epoch: 5\n",
            "\t Training loss 0.00052, Training accuracy 97.91\n",
            "\t Validation loss 0.00036, Validation accuracy 97.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 6\n",
            "\t Training loss 0.00042, Training accuracy 98.27\n",
            "\t Validation loss 0.00033, Validation accuracy 97.52\n",
            "-----------------------------------------------------\n",
            "Epoch: 7\n",
            "\t Training loss 0.00036, Training accuracy 98.40\n",
            "\t Validation loss 0.00029, Validation accuracy 97.70\n",
            "-----------------------------------------------------\n",
            "Epoch: 8\n",
            "\t Training loss 0.00030, Training accuracy 98.72\n",
            "\t Validation loss 0.00024, Validation accuracy 98.11\n",
            "-----------------------------------------------------\n",
            "Epoch: 9\n",
            "\t Training loss 0.00027, Training accuracy 98.91\n",
            "\t Validation loss 0.00024, Validation accuracy 98.12\n",
            "-----------------------------------------------------\n",
            "Epoch: 10\n",
            "\t Training loss 0.00022, Training accuracy 99.08\n",
            "\t Validation loss 0.00025, Validation accuracy 98.12\n",
            "-----------------------------------------------------\n",
            "Epoch: 11\n",
            "\t Training loss 0.00019, Training accuracy 99.19\n",
            "\t Validation loss 0.00026, Validation accuracy 98.06\n",
            "-----------------------------------------------------\n",
            "Epoch: 12\n",
            "\t Training loss 0.00016, Training accuracy 99.32\n",
            "\t Validation loss 0.00023, Validation accuracy 98.30\n",
            "-----------------------------------------------------\n",
            "Epoch: 13\n",
            "\t Training loss 0.00014, Training accuracy 99.44\n",
            "\t Validation loss 0.00024, Validation accuracy 98.30\n",
            "-----------------------------------------------------\n",
            "Epoch: 14\n",
            "\t Training loss 0.00012, Training accuracy 99.53\n",
            "\t Validation loss 0.00022, Validation accuracy 98.39\n",
            "-----------------------------------------------------\n",
            "Epoch: 15\n",
            "\t Training loss 0.00010, Training accuracy 99.60\n",
            "\t Validation loss 0.00022, Validation accuracy 98.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 16\n",
            "\t Training loss 0.00010, Training accuracy 99.63\n",
            "\t Validation loss 0.00026, Validation accuracy 98.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 17\n",
            "\t Training loss 0.00011, Training accuracy 99.54\n",
            "\t Validation loss 0.00026, Validation accuracy 98.21\n",
            "-----------------------------------------------------\n",
            "Epoch: 18\n",
            "\t Training loss 0.00008, Training accuracy 99.67\n",
            "\t Validation loss 0.00026, Validation accuracy 98.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 19\n",
            "\t Training loss 0.00006, Training accuracy 99.74\n",
            "\t Validation loss 0.00026, Validation accuracy 98.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 20\n",
            "\t Training loss 0.00004, Training accuracy 99.88\n",
            "\t Validation loss 0.00024, Validation accuracy 98.57\n",
            "-----------------------------------------------------\n",
            "Epoch: 21\n",
            "\t Training loss 0.00005, Training accuracy 99.84\n",
            "\t Validation loss 0.00028, Validation accuracy 98.29\n",
            "-----------------------------------------------------\n",
            "Epoch: 22\n",
            "\t Training loss 0.00004, Training accuracy 99.83\n",
            "\t Validation loss 0.00026, Validation accuracy 98.52\n",
            "-----------------------------------------------------\n",
            "Epoch: 23\n",
            "\t Training loss 0.00004, Training accuracy 99.87\n",
            "\t Validation loss 0.00025, Validation accuracy 98.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 24\n",
            "\t Training loss 0.00003, Training accuracy 99.88\n",
            "\t Validation loss 0.00026, Validation accuracy 98.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 25\n",
            "\t Training loss 0.00002, Training accuracy 99.97\n",
            "\t Validation loss 0.00026, Validation accuracy 98.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 26\n",
            "\t Training loss 0.00002, Training accuracy 99.95\n",
            "\t Validation loss 0.00026, Validation accuracy 98.52\n",
            "-----------------------------------------------------\n",
            "Epoch: 27\n",
            "\t Training loss 0.00002, Training accuracy 99.93\n",
            "\t Validation loss 0.00028, Validation accuracy 98.46\n",
            "-----------------------------------------------------\n",
            "Epoch: 28\n",
            "\t Training loss 0.00001, Training accuracy 99.99\n",
            "\t Validation loss 0.00027, Validation accuracy 98.57\n",
            "-----------------------------------------------------\n",
            "Epoch: 29\n",
            "\t Training loss 0.00001, Training accuracy 100.00\n",
            "\t Validation loss 0.00028, Validation accuracy 98.59\n",
            "-----------------------------------------------------\n",
            "Epoch: 30\n",
            "\t Training loss 0.00001, Training accuracy 99.99\n",
            "\t Validation loss 0.00028, Validation accuracy 98.66\n",
            "-----------------------------------------------------\n",
            "Epoch: 31\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00028, Validation accuracy 98.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 32\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00028, Validation accuracy 98.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 33\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00028, Validation accuracy 98.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 34\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00028, Validation accuracy 98.64\n",
            "-----------------------------------------------------\n",
            "Epoch: 35\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00029, Validation accuracy 98.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 36\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00029, Validation accuracy 98.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 37\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00029, Validation accuracy 98.63\n",
            "-----------------------------------------------------\n",
            "Epoch: 38\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00029, Validation accuracy 98.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 39\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00030, Validation accuracy 98.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 40\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00030, Validation accuracy 98.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 41\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00030, Validation accuracy 98.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 42\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00030, Validation accuracy 98.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 43\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00030, Validation accuracy 98.58\n",
            "-----------------------------------------------------\n",
            "Epoch: 44\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00030, Validation accuracy 98.63\n",
            "-----------------------------------------------------\n",
            "Epoch: 45\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00031, Validation accuracy 98.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 46\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00030, Validation accuracy 98.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 47\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00031, Validation accuracy 98.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 48\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00031, Validation accuracy 98.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 49\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00031, Validation accuracy 98.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 50\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00031, Validation accuracy 98.61\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00031, Validation accuracy 98.61\n",
            "\t Test loss 0.00022, Test accuracy 98.82\n",
            "-----------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "net = main(device='cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQBDT48CKMVC"
      },
      "source": [
        "Check which MNIST digit is most frequently confused with which other digit (e.g. plot a confusion matrix). Can you explain why?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "1X92ANv6uPea"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def compute_confusion_matrix(model, test_loader, device='cpu'):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    return cm\n",
        "\n",
        "_, _, test_loader = get_data(batch_size=128, test_batch_size=256)\n",
        "# Calcul de la matrice\n",
        "cm = compute_confusion_matrix(net,  test_loader, device='cpu')\n",
        "\n",
        "# Affichage graphique\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion matrix MNIST')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5 and 3 are most frequently confused with which other digit, maybe because they look alike a lot (kinda the same structure and edge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voRqJpgVuQWX"
      },
      "source": [
        "The LeNet5 architecture can also be implemented using the sequential API ([see documentation](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)). Reimplement it with this API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LeNet5(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, kernel_size=5),    # (28 - 5 + 1) = 24\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),       # 28 / 2 = 12\n",
        "\n",
        "            nn.Conv2d(6, 16, kernel_size=5),   # 12 - 5 + 1 = 8\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)        # 8 / 2 = 4\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(16 * 4 * 4, 120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84, 10)                  # 10 classes pour MNIST\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before training:\n",
            "\t Training loss 0.01811, Training accuracy 9.75\n",
            "\t Validation loss 0.00909, Validation accuracy 9.72\n",
            "\t Test loss 0.00925, Test accuracy 9.85\n",
            "-----------------------------------------------------\n",
            "Epoch: 1\n",
            "\t Training loss 0.00959, Training accuracy 56.99\n",
            "\t Validation loss 0.00114, Validation accuracy 90.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 2\n",
            "\t Training loss 0.00136, Training accuracy 94.64\n",
            "\t Validation loss 0.00053, Validation accuracy 95.79\n",
            "-----------------------------------------------------\n",
            "Epoch: 3\n",
            "\t Training loss 0.00078, Training accuracy 96.89\n",
            "\t Validation loss 0.00037, Validation accuracy 97.06\n",
            "-----------------------------------------------------\n",
            "Epoch: 4\n",
            "\t Training loss 0.00059, Training accuracy 97.56\n",
            "\t Validation loss 0.00031, Validation accuracy 97.56\n",
            "-----------------------------------------------------\n",
            "Epoch: 5\n",
            "\t Training loss 0.00049, Training accuracy 98.10\n",
            "\t Validation loss 0.00025, Validation accuracy 98.06\n",
            "-----------------------------------------------------\n",
            "Epoch: 6\n",
            "\t Training loss 0.00041, Training accuracy 98.33\n",
            "\t Validation loss 0.00026, Validation accuracy 97.96\n",
            "-----------------------------------------------------\n",
            "Epoch: 7\n",
            "\t Training loss 0.00035, Training accuracy 98.63\n",
            "\t Validation loss 0.00025, Validation accuracy 98.10\n",
            "-----------------------------------------------------\n",
            "Epoch: 8\n",
            "\t Training loss 0.00029, Training accuracy 98.83\n",
            "\t Validation loss 0.00024, Validation accuracy 98.11\n",
            "-----------------------------------------------------\n",
            "Epoch: 9\n",
            "\t Training loss 0.00026, Training accuracy 98.93\n",
            "\t Validation loss 0.00030, Validation accuracy 97.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 10\n",
            "\t Training loss 0.00021, Training accuracy 99.16\n",
            "\t Validation loss 0.00021, Validation accuracy 98.42\n",
            "-----------------------------------------------------\n",
            "Epoch: 11\n",
            "\t Training loss 0.00018, Training accuracy 99.30\n",
            "\t Validation loss 0.00022, Validation accuracy 98.30\n",
            "-----------------------------------------------------\n",
            "Epoch: 12\n",
            "\t Training loss 0.00016, Training accuracy 99.35\n",
            "\t Validation loss 0.00021, Validation accuracy 98.44\n",
            "-----------------------------------------------------\n",
            "Epoch: 13\n",
            "\t Training loss 0.00015, Training accuracy 99.38\n",
            "\t Validation loss 0.00022, Validation accuracy 98.43\n",
            "-----------------------------------------------------\n",
            "Epoch: 14\n",
            "\t Training loss 0.00012, Training accuracy 99.54\n",
            "\t Validation loss 0.00021, Validation accuracy 98.59\n",
            "-----------------------------------------------------\n",
            "Epoch: 15\n",
            "\t Training loss 0.00010, Training accuracy 99.64\n",
            "\t Validation loss 0.00023, Validation accuracy 98.31\n",
            "-----------------------------------------------------\n",
            "Epoch: 16\n",
            "\t Training loss 0.00009, Training accuracy 99.65\n",
            "\t Validation loss 0.00022, Validation accuracy 98.41\n",
            "-----------------------------------------------------\n",
            "Epoch: 17\n",
            "\t Training loss 0.00009, Training accuracy 99.67\n",
            "\t Validation loss 0.00026, Validation accuracy 98.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 18\n",
            "\t Training loss 0.00008, Training accuracy 99.68\n",
            "\t Validation loss 0.00025, Validation accuracy 98.36\n",
            "-----------------------------------------------------\n",
            "Epoch: 19\n",
            "\t Training loss 0.00006, Training accuracy 99.76\n",
            "\t Validation loss 0.00021, Validation accuracy 98.63\n",
            "-----------------------------------------------------\n",
            "Epoch: 20\n",
            "\t Training loss 0.00005, Training accuracy 99.83\n",
            "\t Validation loss 0.00022, Validation accuracy 98.63\n",
            "-----------------------------------------------------\n",
            "Epoch: 21\n",
            "\t Training loss 0.00005, Training accuracy 99.83\n",
            "\t Validation loss 0.00023, Validation accuracy 98.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 22\n",
            "\t Training loss 0.00004, Training accuracy 99.88\n",
            "\t Validation loss 0.00025, Validation accuracy 98.54\n",
            "-----------------------------------------------------\n",
            "Epoch: 23\n",
            "\t Training loss 0.00004, Training accuracy 99.84\n",
            "\t Validation loss 0.00022, Validation accuracy 98.70\n",
            "-----------------------------------------------------\n",
            "Epoch: 24\n",
            "\t Training loss 0.00002, Training accuracy 99.94\n",
            "\t Validation loss 0.00021, Validation accuracy 98.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 25\n",
            "\t Training loss 0.00002, Training accuracy 99.93\n",
            "\t Validation loss 0.00026, Validation accuracy 98.59\n",
            "-----------------------------------------------------\n",
            "Epoch: 26\n",
            "\t Training loss 0.00001, Training accuracy 99.96\n",
            "\t Validation loss 0.00023, Validation accuracy 98.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 27\n",
            "\t Training loss 0.00002, Training accuracy 99.95\n",
            "\t Validation loss 0.00027, Validation accuracy 98.54\n",
            "-----------------------------------------------------\n",
            "Epoch: 28\n",
            "\t Training loss 0.00003, Training accuracy 99.91\n",
            "\t Validation loss 0.00023, Validation accuracy 98.70\n",
            "-----------------------------------------------------\n",
            "Epoch: 29\n",
            "\t Training loss 0.00002, Training accuracy 99.93\n",
            "\t Validation loss 0.00024, Validation accuracy 98.69\n",
            "-----------------------------------------------------\n",
            "Epoch: 30\n",
            "\t Training loss 0.00002, Training accuracy 99.92\n",
            "\t Validation loss 0.00026, Validation accuracy 98.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 31\n",
            "\t Training loss 0.00002, Training accuracy 99.93\n",
            "\t Validation loss 0.00026, Validation accuracy 98.69\n",
            "-----------------------------------------------------\n",
            "Epoch: 32\n",
            "\t Training loss 0.00001, Training accuracy 99.97\n",
            "\t Validation loss 0.00024, Validation accuracy 98.77\n",
            "-----------------------------------------------------\n",
            "Epoch: 33\n",
            "\t Training loss 0.00001, Training accuracy 99.97\n",
            "\t Validation loss 0.00025, Validation accuracy 98.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 34\n",
            "\t Training loss 0.00001, Training accuracy 99.99\n",
            "\t Validation loss 0.00025, Validation accuracy 98.78\n",
            "-----------------------------------------------------\n",
            "Epoch: 35\n",
            "\t Training loss 0.00001, Training accuracy 99.97\n",
            "\t Validation loss 0.00026, Validation accuracy 98.65\n",
            "-----------------------------------------------------\n",
            "Epoch: 36\n",
            "\t Training loss 0.00001, Training accuracy 99.98\n",
            "\t Validation loss 0.00026, Validation accuracy 98.69\n",
            "-----------------------------------------------------\n",
            "Epoch: 37\n",
            "\t Training loss 0.00001, Training accuracy 99.98\n",
            "\t Validation loss 0.00025, Validation accuracy 98.74\n",
            "-----------------------------------------------------\n",
            "Epoch: 38\n",
            "\t Training loss 0.00000, Training accuracy 99.99\n",
            "\t Validation loss 0.00025, Validation accuracy 98.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 39\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00026, Validation accuracy 98.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 40\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00025, Validation accuracy 98.78\n",
            "-----------------------------------------------------\n",
            "Epoch: 41\n",
            "\t Training loss 0.00000, Training accuracy 99.99\n",
            "\t Validation loss 0.00026, Validation accuracy 98.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 42\n",
            "\t Training loss 0.00000, Training accuracy 99.99\n",
            "\t Validation loss 0.00026, Validation accuracy 98.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 43\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00026, Validation accuracy 98.78\n",
            "-----------------------------------------------------\n",
            "Epoch: 44\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00027, Validation accuracy 98.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 45\n",
            "\t Training loss 0.00000, Training accuracy 99.99\n",
            "\t Validation loss 0.00027, Validation accuracy 98.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 46\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00027, Validation accuracy 98.74\n",
            "-----------------------------------------------------\n",
            "Epoch: 47\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00027, Validation accuracy 98.78\n",
            "-----------------------------------------------------\n",
            "Epoch: 48\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00027, Validation accuracy 98.80\n",
            "-----------------------------------------------------\n",
            "Epoch: 49\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00027, Validation accuracy 98.78\n",
            "-----------------------------------------------------\n",
            "Epoch: 50\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00028, Validation accuracy 98.74\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00028, Validation accuracy 98.74\n",
            "\t Test loss 0.00023, Test accuracy 98.91\n",
            "-----------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "net5=main(device='cpu', lenet='5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i3-pC5xAyu5"
      },
      "source": [
        "## Experiments\n",
        "\n",
        "* Implement adaptive early stopping: if the validation loss did not decrease for K consecutive epochs, stop training.\n",
        "* Change dataset in order to evaluate the LeNet5 network on cifar10 dataset. You can have a look at the pytorch documentation to easily access the cifar10 dataset.\n",
        "* Try to improve performance with:\n",
        "   *   data-augmentation\n",
        "   *   dropout\n",
        "* Implement the resnet18 architecture using the Resnet18 class from pytorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main_early_stopping(batch_size=128,\n",
        "                        device='cpu',\n",
        "                        learning_rate=0.01,\n",
        "                        weight_decay=0.000001,\n",
        "                        momentum=0.9,\n",
        "                        epochs=50,\n",
        "                        lenet='1',\n",
        "                        patience=5): #Patience is the K consecutive epochs before stopping the training\n",
        "\n",
        "  train_loader, val_loader, test_loader = get_data(batch_size)\n",
        "\n",
        "  # Initialisation du rseau\n",
        "  if lenet == '1':\n",
        "    net = LeNet().to(device)\n",
        "  elif lenet == '5':\n",
        "    net = LeNet5().to(device)\n",
        "\n",
        "  else:\n",
        "    print('No net found')\n",
        "    return\n",
        "\n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  best_val_loss = val_loss\n",
        "  best_model_state = None\n",
        "  epochs_without_improvement = 0\n",
        "\n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      best_model_state = net.state_dict()\n",
        "      epochs_without_improvement = 0\n",
        "    else:\n",
        "      epochs_without_improvement += 1\n",
        "      if epochs_without_improvement >= patience:\n",
        "        print(f'Early stopping at epoch {e+1}')\n",
        "        break\n",
        "\n",
        "  # Restaurer le meilleur modle\n",
        "  if best_model_state is not None:\n",
        "    net.load_state_dict(best_model_state)\n",
        "\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  return net\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before training:\n",
            "\t Training loss 0.01805, Training accuracy 9.76\n",
            "\t Validation loss 0.00906, Validation accuracy 10.09\n",
            "\t Test loss 0.00921, Test accuracy 9.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 1\n",
            "\t Training loss 0.00856, Training accuracy 64.85\n",
            "\t Validation loss 0.00094, Validation accuracy 92.59\n",
            "-----------------------------------------------------\n",
            "Epoch: 2\n",
            "\t Training loss 0.00118, Training accuracy 95.24\n",
            "\t Validation loss 0.00040, Validation accuracy 96.79\n",
            "-----------------------------------------------------\n",
            "Epoch: 3\n",
            "\t Training loss 0.00077, Training accuracy 97.02\n",
            "\t Validation loss 0.00041, Validation accuracy 96.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 4\n",
            "\t Training loss 0.00058, Training accuracy 97.62\n",
            "\t Validation loss 0.00028, Validation accuracy 97.70\n",
            "-----------------------------------------------------\n",
            "Epoch: 5\n",
            "\t Training loss 0.00047, Training accuracy 98.09\n",
            "\t Validation loss 0.00025, Validation accuracy 98.03\n",
            "-----------------------------------------------------\n",
            "Epoch: 6\n",
            "\t Training loss 0.00041, Training accuracy 98.32\n",
            "\t Validation loss 0.00023, Validation accuracy 98.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 7\n",
            "\t Training loss 0.00032, Training accuracy 98.71\n",
            "\t Validation loss 0.00023, Validation accuracy 98.27\n",
            "-----------------------------------------------------\n",
            "Epoch: 8\n",
            "\t Training loss 0.00029, Training accuracy 98.82\n",
            "\t Validation loss 0.00022, Validation accuracy 98.34\n",
            "-----------------------------------------------------\n",
            "Epoch: 9\n",
            "\t Training loss 0.00026, Training accuracy 98.95\n",
            "\t Validation loss 0.00020, Validation accuracy 98.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 10\n",
            "\t Training loss 0.00022, Training accuracy 99.10\n",
            "\t Validation loss 0.00024, Validation accuracy 98.19\n",
            "-----------------------------------------------------\n",
            "Epoch: 11\n",
            "\t Training loss 0.00020, Training accuracy 99.16\n",
            "\t Validation loss 0.00019, Validation accuracy 98.55\n",
            "-----------------------------------------------------\n",
            "Epoch: 12\n",
            "\t Training loss 0.00016, Training accuracy 99.37\n",
            "\t Validation loss 0.00021, Validation accuracy 98.45\n",
            "-----------------------------------------------------\n",
            "Epoch: 13\n",
            "\t Training loss 0.00013, Training accuracy 99.49\n",
            "\t Validation loss 0.00019, Validation accuracy 98.59\n",
            "-----------------------------------------------------\n",
            "Epoch: 14\n",
            "\t Training loss 0.00014, Training accuracy 99.46\n",
            "\t Validation loss 0.00020, Validation accuracy 98.57\n",
            "-----------------------------------------------------\n",
            "Epoch: 15\n",
            "\t Training loss 0.00012, Training accuracy 99.48\n",
            "\t Validation loss 0.00021, Validation accuracy 98.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 16\n",
            "\t Training loss 0.00011, Training accuracy 99.55\n",
            "\t Validation loss 0.00021, Validation accuracy 98.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 17\n",
            "\t Training loss 0.00009, Training accuracy 99.65\n",
            "\t Validation loss 0.00020, Validation accuracy 98.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 18\n",
            "\t Training loss 0.00007, Training accuracy 99.71\n",
            "\t Validation loss 0.00022, Validation accuracy 98.53\n",
            "-----------------------------------------------------\n",
            "Early stopping at epoch 18\n",
            "After training:\n",
            "\t Training loss 0.00006, Training accuracy 99.80\n",
            "\t Validation loss 0.00022, Validation accuracy 98.53\n",
            "\t Test loss 0.00018, Test accuracy 98.71\n",
            "-----------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "K=5\n",
        "net5=main_early_stopping(device='cpu', lenet='5', patience=K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation on cifar10 dataset\n",
        "\n",
        "We need to adapt our Net since the cifar10 dataset have 3 input channels (and not only 1 as before) and is 32*32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LeNet5_bis(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5_bis, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            #3d en entre (RGB)\n",
        "            nn.Conv2d(3, 6, kernel_size=5),    # (32 - 5 + 1) = 28\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),       #  28 / 2 = 14\n",
        "\n",
        "            nn.Conv2d(6, 16, kernel_size=5),   # 14 - 5 + 1 = 10\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)        # 10 / 2 = 5\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(16 * 5 * 5, 120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84, 10)                  # 10 classes pour CIFAR\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "New function to obtain the new data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_data2(batch_size, test_batch_size=256):\n",
        "\n",
        "  # Prepare data transformations and then combine them sequentially\n",
        "  transform = list()\n",
        "  transform.append(T.ToTensor())                            # converts Numpy to Pytorch Tensor\n",
        "  transform.append(T.Normalize(mean=[0.5], std=[0.5]))      # Normalizes the Tensors between [-1, 1]\n",
        "  transform = T.Compose(transform)                          # Composes the above transformations into one.\n",
        "\n",
        "  # Load Cifar10 data\n",
        "  full_training_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "  test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "  # Create train and validation splits\n",
        "  num_samples = len(full_training_data)\n",
        "  training_samples = int(num_samples*0.5+1)\n",
        "  validation_samples = num_samples - training_samples\n",
        "\n",
        "  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
        "\n",
        "  # Initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n",
        "\n",
        "  return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also change the main function to have the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main_cifar(batch_size=128,\n",
        "                        device='cpu',\n",
        "                        learning_rate=0.01,\n",
        "                        weight_decay=0.000001,\n",
        "                        momentum=0.9,\n",
        "                        epochs=50,\n",
        "                        lenet='1',\n",
        "                        patience=5): #Patience is the K consecutive epochs before stopping the training\n",
        "\n",
        "  train_loader, val_loader, test_loader = get_data2(batch_size)\n",
        "\n",
        "  # Initialisation du rseau\n",
        "  if lenet == '1':\n",
        "    net = LeNet().to(device)\n",
        "  elif lenet == '5':\n",
        "    net = LeNet5().to(device)\n",
        "  elif lenet=='5bis':\n",
        "    net = LeNet5_bis().to(device)\n",
        "\n",
        "  else:\n",
        "    print('No net found')\n",
        "    return\n",
        "\n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  best_val_loss = val_loss\n",
        "  best_model_state = None\n",
        "  epochs_without_improvement = 0\n",
        "\n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      best_model_state = net.state_dict()\n",
        "      epochs_without_improvement = 0\n",
        "    else:\n",
        "      epochs_without_improvement += 1\n",
        "      if epochs_without_improvement >= patience:\n",
        "        print(f'Early stopping at epoch {e+1}')\n",
        "        break\n",
        "\n",
        "  # Restaurer le meilleur modle\n",
        "  if best_model_state is not None:\n",
        "    net.load_state_dict(best_model_state)\n",
        "\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  return net\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50000\n",
            "Before training:\n",
            "\t Training loss 0.01808, Training accuracy 10.12\n",
            "\t Validation loss 0.00905, Validation accuracy 9.90\n",
            "\t Test loss 0.00923, Test accuracy 10.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 1\n",
            "\t Training loss 0.01619, Training accuracy 23.64\n",
            "\t Validation loss 0.00694, Validation accuracy 35.83\n",
            "-----------------------------------------------------\n",
            "Epoch: 2\n",
            "\t Training loss 0.01274, Training accuracy 40.19\n",
            "\t Validation loss 0.00591, Validation accuracy 45.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 3\n",
            "\t Training loss 0.01140, Training accuracy 46.87\n",
            "\t Validation loss 0.00557, Validation accuracy 48.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 4\n",
            "\t Training loss 0.01055, Training accuracy 51.25\n",
            "\t Validation loss 0.00527, Validation accuracy 51.77\n",
            "-----------------------------------------------------\n",
            "Epoch: 5\n",
            "\t Training loss 0.00991, Training accuracy 54.60\n",
            "\t Validation loss 0.00510, Validation accuracy 53.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 6\n",
            "\t Training loss 0.00940, Training accuracy 57.16\n",
            "\t Validation loss 0.00483, Validation accuracy 55.92\n",
            "-----------------------------------------------------\n",
            "Epoch: 7\n",
            "\t Training loss 0.00893, Training accuracy 59.28\n",
            "\t Validation loss 0.00482, Validation accuracy 56.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 8\n",
            "\t Training loss 0.00851, Training accuracy 61.52\n",
            "\t Validation loss 0.00465, Validation accuracy 58.58\n",
            "-----------------------------------------------------\n",
            "Epoch: 9\n",
            "\t Training loss 0.00801, Training accuracy 63.80\n",
            "\t Validation loss 0.00466, Validation accuracy 58.34\n",
            "-----------------------------------------------------\n",
            "Epoch: 10\n",
            "\t Training loss 0.00765, Training accuracy 65.25\n",
            "\t Validation loss 0.00480, Validation accuracy 57.70\n",
            "-----------------------------------------------------\n",
            "Epoch: 11\n",
            "\t Training loss 0.00734, Training accuracy 66.81\n",
            "\t Validation loss 0.00465, Validation accuracy 59.11\n",
            "-----------------------------------------------------\n",
            "Epoch: 12\n",
            "\t Training loss 0.00694, Training accuracy 68.73\n",
            "\t Validation loss 0.00479, Validation accuracy 58.78\n",
            "-----------------------------------------------------\n",
            "Epoch: 13\n",
            "\t Training loss 0.00655, Training accuracy 70.52\n",
            "\t Validation loss 0.00484, Validation accuracy 58.54\n",
            "-----------------------------------------------------\n",
            "Epoch: 14\n",
            "\t Training loss 0.00629, Training accuracy 71.35\n",
            "\t Validation loss 0.00479, Validation accuracy 59.44\n",
            "-----------------------------------------------------\n",
            "Epoch: 15\n",
            "\t Training loss 0.00589, Training accuracy 73.22\n",
            "\t Validation loss 0.00501, Validation accuracy 59.35\n",
            "-----------------------------------------------------\n",
            "Epoch: 16\n",
            "\t Training loss 0.00561, Training accuracy 74.42\n",
            "\t Validation loss 0.00522, Validation accuracy 58.32\n",
            "-----------------------------------------------------\n",
            "Epoch: 17\n",
            "\t Training loss 0.00535, Training accuracy 75.42\n",
            "\t Validation loss 0.00519, Validation accuracy 58.09\n",
            "-----------------------------------------------------\n",
            "Epoch: 18\n",
            "\t Training loss 0.00496, Training accuracy 77.17\n",
            "\t Validation loss 0.00554, Validation accuracy 58.79\n",
            "-----------------------------------------------------\n",
            "Early stopping at epoch 18\n",
            "After training:\n",
            "\t Training loss 0.00420, Training accuracy 81.20\n",
            "\t Validation loss 0.00554, Validation accuracy 58.79\n",
            "\t Test loss 0.00568, Test accuracy 58.78\n",
            "-----------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "K=10\n",
        "netcifar=main_cifar(device='cpu', lenet='5bis', patience=K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50000\n",
            "Before training:\n",
            "\t Training loss 0.01807, Training accuracy 9.94\n",
            "\t Validation loss 0.00904, Validation accuracy 10.10\n",
            "\t Test loss 0.00922, Test accuracy 10.05\n",
            "-----------------------------------------------------\n",
            "Epoch: 1\n",
            "\t Training loss 0.01624, Training accuracy 23.22\n",
            "\t Validation loss 0.00691, Validation accuracy 36.96\n",
            "-----------------------------------------------------\n",
            "Epoch: 2\n",
            "\t Training loss 0.01285, Training accuracy 40.00\n",
            "\t Validation loss 0.00623, Validation accuracy 42.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 3\n",
            "\t Training loss 0.01160, Training accuracy 45.90\n",
            "\t Validation loss 0.00570, Validation accuracy 46.27\n",
            "-----------------------------------------------------\n",
            "Epoch: 4\n",
            "\t Training loss 0.01073, Training accuracy 50.15\n",
            "\t Validation loss 0.00527, Validation accuracy 51.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 5\n",
            "\t Training loss 0.01018, Training accuracy 53.28\n",
            "\t Validation loss 0.00507, Validation accuracy 54.21\n",
            "-----------------------------------------------------\n",
            "Epoch: 6\n",
            "\t Training loss 0.00948, Training accuracy 56.77\n",
            "\t Validation loss 0.00516, Validation accuracy 54.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 7\n",
            "\t Training loss 0.00910, Training accuracy 58.32\n",
            "\t Validation loss 0.00483, Validation accuracy 56.42\n",
            "-----------------------------------------------------\n",
            "Epoch: 8\n",
            "\t Training loss 0.00862, Training accuracy 60.63\n",
            "\t Validation loss 0.00472, Validation accuracy 57.95\n",
            "-----------------------------------------------------\n",
            "Epoch: 9\n",
            "\t Training loss 0.00827, Training accuracy 62.47\n",
            "\t Validation loss 0.00471, Validation accuracy 57.84\n",
            "-----------------------------------------------------\n",
            "Epoch: 10\n",
            "\t Training loss 0.00784, Training accuracy 64.65\n",
            "\t Validation loss 0.00472, Validation accuracy 58.08\n",
            "-----------------------------------------------------\n",
            "Epoch: 11\n",
            "\t Training loss 0.00754, Training accuracy 66.14\n",
            "\t Validation loss 0.00461, Validation accuracy 59.69\n",
            "-----------------------------------------------------\n",
            "Epoch: 12\n",
            "\t Training loss 0.00718, Training accuracy 67.47\n",
            "\t Validation loss 0.00474, Validation accuracy 58.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 13\n",
            "\t Training loss 0.00684, Training accuracy 69.36\n",
            "\t Validation loss 0.00464, Validation accuracy 59.63\n",
            "-----------------------------------------------------\n",
            "Epoch: 14\n",
            "\t Training loss 0.00644, Training accuracy 71.12\n",
            "\t Validation loss 0.00497, Validation accuracy 58.95\n",
            "-----------------------------------------------------\n",
            "Epoch: 15\n",
            "\t Training loss 0.00603, Training accuracy 72.87\n",
            "\t Validation loss 0.00513, Validation accuracy 57.86\n",
            "-----------------------------------------------------\n",
            "Epoch: 16\n",
            "\t Training loss 0.00584, Training accuracy 73.53\n",
            "\t Validation loss 0.00491, Validation accuracy 60.52\n",
            "-----------------------------------------------------\n",
            "Epoch: 17\n",
            "\t Training loss 0.00543, Training accuracy 75.49\n",
            "\t Validation loss 0.00534, Validation accuracy 59.35\n",
            "-----------------------------------------------------\n",
            "Epoch: 18\n",
            "\t Training loss 0.00519, Training accuracy 76.43\n",
            "\t Validation loss 0.00531, Validation accuracy 59.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 19\n",
            "\t Training loss 0.00494, Training accuracy 77.53\n",
            "\t Validation loss 0.00572, Validation accuracy 58.27\n",
            "-----------------------------------------------------\n",
            "Epoch: 20\n",
            "\t Training loss 0.00452, Training accuracy 79.56\n",
            "\t Validation loss 0.00576, Validation accuracy 58.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 21\n",
            "\t Training loss 0.00433, Training accuracy 80.29\n",
            "\t Validation loss 0.00598, Validation accuracy 59.10\n",
            "-----------------------------------------------------\n",
            "Early stopping at epoch 21\n",
            "After training:\n",
            "\t Training loss 0.00329, Training accuracy 85.31\n",
            "\t Validation loss 0.00598, Validation accuracy 59.10\n",
            "\t Test loss 0.00612, Test accuracy 58.95\n",
            "-----------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def get_data2(batch_size, test_batch_size=256):\n",
        "    # Data augmentation for training data\n",
        "    transform_train = T.Compose([\n",
        "        #Random transformations at each epochs\n",
        "        T.RandomHorizontalFlip(),                 #Random horizontal flip\n",
        "        T.RandomRotation(degrees=10),             #Random rotation\n",
        "\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                    std=[0.2023, 0.1994, 0.2010])  # Standard CIFAR-10 normalization\n",
        "    ])\n",
        "\n",
        "    # No augmentation for validation and test sets\n",
        "    transform_test = T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                    std=[0.2023, 0.1994, 0.2010])\n",
        "    ])\n",
        "\n",
        "    # Load CIFAR-10 dataset\n",
        "    full_training_data = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=True, download=True, transform=transform_train)\n",
        "    \n",
        "    test_data = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "    # Create train and validation splits\n",
        "    num_samples = len(full_training_data)\n",
        "    training_samples = int(num_samples * 0.5 + 1)\n",
        "    validation_samples = num_samples - training_samples\n",
        "\n",
        "    training_data, validation_data = torch.utils.data.random_split(\n",
        "        full_training_data, [training_samples, validation_samples])\n",
        "\n",
        "    # Apply test transforms to validation data\n",
        "    validation_data.dataset.transform = transform_test\n",
        "\n",
        "    # Initialize dataloaders\n",
        "    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# Train : \n",
        "\n",
        "netcifar_bis= main_cifar(device='cpu', lenet='5bis', patience=K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LeNet5_bis(torch.nn.Module):\n",
        "    def __init__(self, dropout_p=0.4):\n",
        "        super(LeNet5_bis, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            #3d en entre (RGB)\n",
        "            nn.Conv2d(3, 6, kernel_size=5),    # (32 - 5 + 1) = 28\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),       #  28 / 2 = 14\n",
        "\n",
        "            nn.Conv2d(6, 16, kernel_size=5),   # 14 - 5 + 1 = 10\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)        # 10 / 2 = 5\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(16 * 5 * 5, 120),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout_p),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout_p), \n",
        "            nn.Linear(84, 10)                  # 10 classes pour CIFAR\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50000\n",
            "Before training:\n",
            "\t Training loss 0.01807, Training accuracy 9.97\n",
            "\t Validation loss 0.00904, Validation accuracy 9.97\n",
            "\t Test loss 0.00922, Test accuracy 9.81\n",
            "-----------------------------------------------------\n",
            "Epoch: 1\n",
            "\t Training loss 0.01712, Training accuracy 17.92\n",
            "\t Validation loss 0.00750, Validation accuracy 29.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 2\n",
            "\t Training loss 0.01443, Training accuracy 31.31\n",
            "\t Validation loss 0.00667, Validation accuracy 38.19\n",
            "-----------------------------------------------------\n",
            "Epoch: 3\n",
            "\t Training loss 0.01315, Training accuracy 38.21\n",
            "\t Validation loss 0.00604, Validation accuracy 44.09\n",
            "-----------------------------------------------------\n",
            "Epoch: 4\n",
            "\t Training loss 0.01236, Training accuracy 42.32\n",
            "\t Validation loss 0.00573, Validation accuracy 47.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 5\n",
            "\t Training loss 0.01181, Training accuracy 45.01\n",
            "\t Validation loss 0.00553, Validation accuracy 48.37\n",
            "-----------------------------------------------------\n",
            "Epoch: 6\n",
            "\t Training loss 0.01140, Training accuracy 47.43\n",
            "\t Validation loss 0.00542, Validation accuracy 50.69\n",
            "-----------------------------------------------------\n",
            "Epoch: 7\n",
            "\t Training loss 0.01100, Training accuracy 49.26\n",
            "\t Validation loss 0.00528, Validation accuracy 51.90\n",
            "-----------------------------------------------------\n",
            "Epoch: 8\n",
            "\t Training loss 0.01066, Training accuracy 50.93\n",
            "\t Validation loss 0.00496, Validation accuracy 54.36\n",
            "-----------------------------------------------------\n",
            "Epoch: 9\n",
            "\t Training loss 0.01030, Training accuracy 52.90\n",
            "\t Validation loss 0.00487, Validation accuracy 55.34\n",
            "-----------------------------------------------------\n",
            "Epoch: 10\n",
            "\t Training loss 0.01020, Training accuracy 53.82\n",
            "\t Validation loss 0.00509, Validation accuracy 54.41\n",
            "-----------------------------------------------------\n",
            "Epoch: 11\n",
            "\t Training loss 0.00988, Training accuracy 54.77\n",
            "\t Validation loss 0.00479, Validation accuracy 56.63\n",
            "-----------------------------------------------------\n",
            "Epoch: 12\n",
            "\t Training loss 0.00968, Training accuracy 55.97\n",
            "\t Validation loss 0.00492, Validation accuracy 55.12\n",
            "-----------------------------------------------------\n",
            "Epoch: 13\n",
            "\t Training loss 0.00951, Training accuracy 56.70\n",
            "\t Validation loss 0.00461, Validation accuracy 58.32\n",
            "-----------------------------------------------------\n",
            "Epoch: 14\n",
            "\t Training loss 0.00926, Training accuracy 57.98\n",
            "\t Validation loss 0.00465, Validation accuracy 58.31\n",
            "-----------------------------------------------------\n",
            "Epoch: 15\n",
            "\t Training loss 0.00905, Training accuracy 58.91\n",
            "\t Validation loss 0.00470, Validation accuracy 58.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 16\n",
            "\t Training loss 0.00891, Training accuracy 59.81\n",
            "\t Validation loss 0.00459, Validation accuracy 58.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 17\n",
            "\t Training loss 0.00877, Training accuracy 60.12\n",
            "\t Validation loss 0.00449, Validation accuracy 59.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 18\n",
            "\t Training loss 0.00864, Training accuracy 60.99\n",
            "\t Validation loss 0.00458, Validation accuracy 59.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 19\n",
            "\t Training loss 0.00856, Training accuracy 61.11\n",
            "\t Validation loss 0.00463, Validation accuracy 59.23\n",
            "-----------------------------------------------------\n",
            "Epoch: 20\n",
            "\t Training loss 0.00836, Training accuracy 62.37\n",
            "\t Validation loss 0.00449, Validation accuracy 60.23\n",
            "-----------------------------------------------------\n",
            "Epoch: 21\n",
            "\t Training loss 0.00818, Training accuracy 62.77\n",
            "\t Validation loss 0.00447, Validation accuracy 60.51\n",
            "-----------------------------------------------------\n",
            "Epoch: 22\n",
            "\t Training loss 0.00814, Training accuracy 62.71\n",
            "\t Validation loss 0.00456, Validation accuracy 59.86\n",
            "-----------------------------------------------------\n",
            "Epoch: 23\n",
            "\t Training loss 0.00801, Training accuracy 63.58\n",
            "\t Validation loss 0.00443, Validation accuracy 60.35\n",
            "-----------------------------------------------------\n",
            "Epoch: 24\n",
            "\t Training loss 0.00785, Training accuracy 64.12\n",
            "\t Validation loss 0.00457, Validation accuracy 60.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 25\n",
            "\t Training loss 0.00790, Training accuracy 64.13\n",
            "\t Validation loss 0.00455, Validation accuracy 59.21\n",
            "-----------------------------------------------------\n",
            "Epoch: 26\n",
            "\t Training loss 0.00774, Training accuracy 64.95\n",
            "\t Validation loss 0.00454, Validation accuracy 60.56\n",
            "-----------------------------------------------------\n",
            "Epoch: 27\n",
            "\t Training loss 0.00767, Training accuracy 65.29\n",
            "\t Validation loss 0.00448, Validation accuracy 60.52\n",
            "-----------------------------------------------------\n",
            "Epoch: 28\n",
            "\t Training loss 0.00747, Training accuracy 66.00\n",
            "\t Validation loss 0.00446, Validation accuracy 61.09\n",
            "-----------------------------------------------------\n",
            "Epoch: 29\n",
            "\t Training loss 0.00748, Training accuracy 65.62\n",
            "\t Validation loss 0.00447, Validation accuracy 61.17\n",
            "-----------------------------------------------------\n",
            "Epoch: 30\n",
            "\t Training loss 0.00734, Training accuracy 66.62\n",
            "\t Validation loss 0.00456, Validation accuracy 60.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 31\n",
            "\t Training loss 0.00727, Training accuracy 66.73\n",
            "\t Validation loss 0.00458, Validation accuracy 60.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 32\n",
            "\t Training loss 0.00724, Training accuracy 67.34\n",
            "\t Validation loss 0.00458, Validation accuracy 60.64\n",
            "-----------------------------------------------------\n",
            "Epoch: 33\n",
            "\t Training loss 0.00706, Training accuracy 67.65\n",
            "\t Validation loss 0.00455, Validation accuracy 60.56\n",
            "-----------------------------------------------------\n",
            "Early stopping at epoch 33\n",
            "After training:\n",
            "\t Training loss 0.00556, Training accuracy 75.20\n",
            "\t Validation loss 0.00455, Validation accuracy 60.56\n",
            "\t Test loss 0.00470, Test accuracy 60.03\n",
            "-----------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "netcifar_dropout= main_cifar(device='cpu', lenet='5bis', patience=K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Resnet18 architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class ResNet18_CIFAR10(nn.Module):\n",
        "    def __init__(self, pretrained=False):\n",
        "        super(ResNet18_CIFAR10, self).__init__()\n",
        "        self.model = torchvision.models.resnet18(pretrained=pretrained)\n",
        "\n",
        "        # Adapt the first layer to CIFAR-10 (32x32 images)\n",
        "        self.model.conv1 = nn.Conv2d(\n",
        "            3, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
        "        )\n",
        "        self.model.maxpool = nn.Identity()  # removing maxpooling cause too early\n",
        "\n",
        "        # Adapt the last layer to 10 classes classification\n",
        "        num_ftrs = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main_resnet18(batch_size=128,\n",
        "                        device='cpu',\n",
        "                        learning_rate=0.01,\n",
        "                        weight_decay=0.000001,\n",
        "                        momentum=0.9,\n",
        "                        epochs=50,\n",
        "                        patience=5): #Patience is the K consecutive epochs before stopping the training\n",
        "\n",
        "  train_loader, val_loader, test_loader = get_data2(batch_size)\n",
        "\n",
        "  net = ResNet18_CIFAR10().to(device)\n",
        "\n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  best_val_loss = val_loss\n",
        "  best_model_state = None\n",
        "  epochs_without_improvement = 0\n",
        "\n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      best_model_state = net.state_dict()\n",
        "      epochs_without_improvement = 0\n",
        "    else:\n",
        "      epochs_without_improvement += 1\n",
        "      if epochs_without_improvement >= patience:\n",
        "        print(f'Early stopping at epoch {e+1}')\n",
        "        break\n",
        "\n",
        "  # Restaurer le meilleur modle\n",
        "  if best_model_state is not None:\n",
        "    net.load_state_dict(best_model_state)\n",
        "\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  return net\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It takes way to much time but already at the first epoch, the loss was low"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50000\n",
            "Before training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t Training loss 0.02956, Training accuracy 9.79\n",
            "\t Validation loss 0.01476, Validation accuracy 10.21\n",
            "\t Test loss 0.01519, Test accuracy 10.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 1\n",
            "\t Training loss 0.01280, Training accuracy 39.38\n",
            "\t Validation loss 0.00623, Validation accuracy 45.15\n",
            "-----------------------------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[105], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m final_net \u001b[38;5;241m=\u001b[39m \u001b[43mmain_resnet18\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[104], line 31\u001b[0m, in \u001b[0;36mmain_resnet18\u001b[1;34m(batch_size, device, learning_rate, weight_decay, momentum, epochs, patience)\u001b[0m\n\u001b[0;32m     28\u001b[0m epochs_without_improvement \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 31\u001b[0m   train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcost_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m   val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m test(net, val_loader, cost_function)\n\u001b[0;32m     34\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{:d}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
            "Cell \u001b[1;32mIn[38], line 49\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, data_loader, optimizer, cost_function, device)\u001b[0m\n\u001b[0;32m     44\u001b[0m loss \u001b[38;5;241m=\u001b[39m cost_function(outputs,targets)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Reset the optimizer\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[0;32m     52\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[1;32mc:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "final_net = main_resnet18(device='cpu')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
